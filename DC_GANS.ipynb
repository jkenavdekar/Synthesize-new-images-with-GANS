{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DC_GANS.ipynb","provenance":[],"authorship_tag":"ABX9TyP6BaAynrvibD8xyJBd9hG2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7e851e4cecf24a10af2d6575262fac7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_28c8160208ce4b1c9a0d9bfaf00b5c8f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ee2b56f1dc4c4b80809b2a221cc9dbd5","IPY_MODEL_8f587f53668549a48fc0d7bb0ebc55c6","IPY_MODEL_d4f89e9efee2475aaf12a3f056ec7455"]}},"28c8160208ce4b1c9a0d9bfaf00b5c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ee2b56f1dc4c4b80809b2a221cc9dbd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ca033e306a84ecf8caa49df8a83f658","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_23f5262c679b46f5bdb341a20548ff88"}},"8f587f53668549a48fc0d7bb0ebc55c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5747360fe1c04d4dafadb6d6decd63ce","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_171409806ba04506a8f29784937e382e"}},"d4f89e9efee2475aaf12a3f056ec7455":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1a70c9ce97aa438d8c19ac0a5e882fa0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:06&lt;00:00, 31061241.44it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_48e45f31f10843f69236380c20103dbb"}},"0ca033e306a84ecf8caa49df8a83f658":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"23f5262c679b46f5bdb341a20548ff88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5747360fe1c04d4dafadb6d6decd63ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"171409806ba04506a8f29784937e382e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a70c9ce97aa438d8c19ac0a5e882fa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"48e45f31f10843f69236380c20103dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XR_ug_xBsi96","executionInfo":{"status":"ok","timestamp":1629549141455,"user_tz":-120,"elapsed":25000,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}},"outputId":"0c99b0a4-e5df-4fa0-9cd7-184e3177754d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_4OdnktsqDp","executionInfo":{"status":"ok","timestamp":1629549178306,"user_tz":-120,"elapsed":513,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}},"outputId":"8c746117-d8e2-4e2e-eefd-487948b87e93"},"source":["%cd /content/drive/My Drive/GANS/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/GANS\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W9eQlfqnrXN6","executionInfo":{"status":"ok","timestamp":1629548926888,"user_tz":-120,"elapsed":3375,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}}},"source":["# Deep Convolutional GANs\n","\n","# Importing the libraries\n","from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","from torch.autograd import Variable\n","\n","# Setting some hyperparameters\n","batchSize = 64 # We set the size of the batch.\n","imageSize = 64 # We set the size of the generated images (64x64)."],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["7e851e4cecf24a10af2d6575262fac7d","28c8160208ce4b1c9a0d9bfaf00b5c8f","ee2b56f1dc4c4b80809b2a221cc9dbd5","8f587f53668549a48fc0d7bb0ebc55c6","d4f89e9efee2475aaf12a3f056ec7455","0ca033e306a84ecf8caa49df8a83f658","23f5262c679b46f5bdb341a20548ff88","5747360fe1c04d4dafadb6d6decd63ce","171409806ba04506a8f29784937e382e","1a70c9ce97aa438d8c19ac0a5e882fa0","48e45f31f10843f69236380c20103dbb"]},"id":"w-c5rTAvr2vR","executionInfo":{"status":"ok","timestamp":1629549194459,"user_tz":-120,"elapsed":11547,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}},"outputId":"c74254ca-7c84-489e-b9be-a21ede5daa4e"},"source":["# Creating the transformations\n","transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n","\n","# Loading the dataset\n","dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.\n","\n","# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:310: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e851e4cecf24a10af2d6575262fac7d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8anUf3Its4ox","executionInfo":{"status":"ok","timestamp":1629549557410,"user_tz":-120,"elapsed":512,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}}},"source":["# Defining the generator\n","\n","class G(nn.Module): # We introduce a class to define the generator.\n","\n","    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.\n","        super(G, self).__init__() # We inherit from the nn.Module tools.\n","        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n","            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # We start with an inversed convolution.\n","            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.\n","            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution.\n","            nn.BatchNorm2d(256), # We normalize again.\n","            nn.ReLU(True), # We apply another ReLU.\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution.\n","            nn.BatchNorm2d(128), # We normalize again.\n","            nn.ReLU(True), # We apply another ReLU.\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution.\n","            nn.BatchNorm2d(64), # We normalize again.\n","            nn.ReLU(True), # We apply another ReLU.\n","            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution.\n","            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.\n","        )\n","\n","    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.\n","        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.\n","        return output # We return the output containing the generated images."],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xoim_x3NuRlS","executionInfo":{"status":"ok","timestamp":1629549574834,"user_tz":-120,"elapsed":546,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}},"outputId":"bc7eeef3-8657-40a8-817f-65e5e7929c6b"},"source":["# Creating the generator\n","netG = G() # We create the generator object.\n","netG.apply(weights_init) # We initialize all the weights of its neural network.\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["G(\n","  (main): Sequential(\n","    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (13): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"OTwQnEYfuXU5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xU_LrzGauR0S","executionInfo":{"status":"ok","timestamp":1629549636778,"user_tz":-120,"elapsed":302,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}}},"source":["# Defining the discriminator\n","\n","class D(nn.Module): # We introduce a class to define the discriminator.\n","\n","    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.\n","        super(D, self).__init__() # We inherit from the nn.Module tools.\n","        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n","            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.\n","            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.\n","            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.\n","            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.\n","            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n","            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.\n","            nn.BatchNorm2d(256), # We normalize again.\n","            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n","            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.\n","            nn.BatchNorm2d(512), # We normalize again.\n","            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n","            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.\n","            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n","        )\n","\n","    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.\n","        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.\n","        return output.view(-1) # We return the output which will be a value between 0 and 1."],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QdQCow0cuR6Y","executionInfo":{"status":"ok","timestamp":1629549675329,"user_tz":-120,"elapsed":301,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}},"outputId":"502c6f17-1d90-40ef-b0e7-da26e2d5db10"},"source":["# Creating the discriminator\n","netD = D() # We create the discriminator object.\n","netD.apply(weights_init) # We initialize all the weights of its neural network."],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["D(\n","  (main): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (12): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"n6Ya0IpVuR-L","executionInfo":{"status":"ok","timestamp":1629549735942,"user_tz":-120,"elapsed":286,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}}},"source":["# Training the DCGANs\n","\n","criterion = nn.BCELoss() # We create a criterion object that will measure the error between the prediction and the target.\n","optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.\n","optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator."],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xoGvJ93AuSBw","executionInfo":{"status":"error","timestamp":1629556136262,"user_tz":-120,"elapsed":3252857,"user":{"displayName":"jayesh kenavdekar","photoUrl":"","userId":"10372042945182182273"}},"outputId":"836e9e3f-a4e1-4af8-e58b-5fac7bc639ef"},"source":["for epoch in range(2):\n","\n","    for i, data in enumerate(dataloader, 0):\n","        \n","        # 1st Step: Updating the weights of the neural network of the discriminator\n","\n","        netD.zero_grad()\n","        \n","        # Training the discriminator with a real image of the dataset\n","        real, _ = data\n","        input = Variable(real)\n","        target = Variable(torch.ones(input.size()[0]))\n","        output = netD(input)\n","        errD_real = criterion(output, target)\n","        \n","        # Training the discriminator with a fake image generated by the generator\n","        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n","        fake = netG(noise)\n","        target = Variable(torch.zeros(input.size()[0]))\n","        output = netD(fake.detach())\n","        errD_fake = criterion(output, target)\n","        \n","        # Backpropagating the total error\n","        errD = errD_real + errD_fake\n","        errD.backward()\n","        optimizerD.step()\n","\n","        # 2nd Step: Updating the weights of the neural network of the generator\n","\n","        netG.zero_grad()\n","        target = Variable(torch.ones(input.size()[0]))\n","        output = netD(fake)\n","        errG = criterion(output, target)\n","        errG.backward()\n","        optimizerG.step()\n","        \n","        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n","\n","        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 2, i, len(dataloader), errD.item(), errG.item()))\n","        if i % 100 == 0:\n","            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n","            fake = netG(noise)\n","            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[0/2][0/782] Loss_D: 0.5433 Loss_G: 22.6936\n","[0/2][1/782] Loss_D: 0.2521 Loss_G: 18.3934\n","[0/2][2/782] Loss_D: 0.0722 Loss_G: 12.0668\n","[0/2][3/782] Loss_D: 0.0532 Loss_G: 5.4558\n","[0/2][4/782] Loss_D: 1.8552 Loss_G: 19.8222\n","[0/2][5/782] Loss_D: 0.3928 Loss_G: 22.4991\n","[0/2][6/782] Loss_D: 0.1182 Loss_G: 19.7310\n","[0/2][7/782] Loss_D: 0.3983 Loss_G: 11.2658\n","[0/2][8/782] Loss_D: 0.7018 Loss_G: 13.8880\n","[0/2][9/782] Loss_D: 0.0944 Loss_G: 12.1632\n","[0/2][10/782] Loss_D: 0.1700 Loss_G: 8.0219\n","[0/2][11/782] Loss_D: 0.6277 Loss_G: 18.6068\n","[0/2][12/782] Loss_D: 1.8395 Loss_G: 13.3272\n","[0/2][13/782] Loss_D: 0.0611 Loss_G: 5.7580\n","[0/2][14/782] Loss_D: 2.2390 Loss_G: 21.5098\n","[0/2][15/782] Loss_D: 1.3560 Loss_G: 20.4471\n","[0/2][16/782] Loss_D: 0.2731 Loss_G: 13.4756\n","[0/2][17/782] Loss_D: 0.3261 Loss_G: 5.7753\n","[0/2][18/782] Loss_D: 2.4284 Loss_G: 15.6442\n","[0/2][19/782] Loss_D: 0.8181 Loss_G: 16.1647\n","[0/2][20/782] Loss_D: 0.4339 Loss_G: 12.3753\n","[0/2][21/782] Loss_D: 0.1772 Loss_G: 5.9115\n","[0/2][22/782] Loss_D: 1.0556 Loss_G: 10.5283\n","[0/2][23/782] Loss_D: 0.5952 Loss_G: 9.8408\n","[0/2][24/782] Loss_D: 0.2720 Loss_G: 6.3646\n","[0/2][25/782] Loss_D: 0.2403 Loss_G: 4.9537\n","[0/2][26/782] Loss_D: 0.6054 Loss_G: 9.8795\n","[0/2][27/782] Loss_D: 0.3446 Loss_G: 8.3698\n","[0/2][28/782] Loss_D: 0.2270 Loss_G: 5.3018\n","[0/2][29/782] Loss_D: 0.8166 Loss_G: 10.5332\n","[0/2][30/782] Loss_D: 0.4835 Loss_G: 9.0540\n","[0/2][31/782] Loss_D: 0.3244 Loss_G: 4.8435\n","[0/2][32/782] Loss_D: 1.5873 Loss_G: 14.9786\n","[0/2][33/782] Loss_D: 1.6321 Loss_G: 11.5093\n","[0/2][34/782] Loss_D: 0.1997 Loss_G: 6.2414\n","[0/2][35/782] Loss_D: 0.4905 Loss_G: 6.5374\n","[0/2][36/782] Loss_D: 0.4297 Loss_G: 8.4224\n","[0/2][37/782] Loss_D: 0.6922 Loss_G: 6.4935\n","[0/2][38/782] Loss_D: 0.2358 Loss_G: 5.1156\n","[0/2][39/782] Loss_D: 0.4697 Loss_G: 5.7180\n","[0/2][40/782] Loss_D: 0.2824 Loss_G: 5.3340\n","[0/2][41/782] Loss_D: 0.2672 Loss_G: 5.2127\n","[0/2][42/782] Loss_D: 0.3701 Loss_G: 3.9126\n","[0/2][43/782] Loss_D: 0.5806 Loss_G: 7.6928\n","[0/2][44/782] Loss_D: 0.9614 Loss_G: 3.0838\n","[0/2][45/782] Loss_D: 0.6219 Loss_G: 7.6327\n","[0/2][46/782] Loss_D: 0.4216 Loss_G: 6.0952\n","[0/2][47/782] Loss_D: 0.2831 Loss_G: 3.7558\n","[0/2][48/782] Loss_D: 0.5040 Loss_G: 6.3835\n","[0/2][49/782] Loss_D: 0.2309 Loss_G: 5.5563\n","[0/2][50/782] Loss_D: 0.1957 Loss_G: 4.4537\n","[0/2][51/782] Loss_D: 0.3277 Loss_G: 4.5683\n","[0/2][52/782] Loss_D: 0.4459 Loss_G: 4.6847\n","[0/2][53/782] Loss_D: 0.2869 Loss_G: 4.6068\n","[0/2][54/782] Loss_D: 0.5115 Loss_G: 3.8701\n","[0/2][55/782] Loss_D: 0.7358 Loss_G: 4.9082\n","[0/2][56/782] Loss_D: 0.3220 Loss_G: 5.5533\n","[0/2][57/782] Loss_D: 0.1940 Loss_G: 5.0792\n","[0/2][58/782] Loss_D: 0.3822 Loss_G: 5.4842\n","[0/2][59/782] Loss_D: 0.3258 Loss_G: 4.5521\n","[0/2][60/782] Loss_D: 0.4878 Loss_G: 7.2905\n","[0/2][61/782] Loss_D: 0.7008 Loss_G: 1.0836\n","[0/2][62/782] Loss_D: 1.5914 Loss_G: 14.4500\n","[0/2][63/782] Loss_D: 3.6379 Loss_G: 10.4267\n","[0/2][64/782] Loss_D: 0.4612 Loss_G: 5.6975\n","[0/2][65/782] Loss_D: 0.2141 Loss_G: 3.3268\n","[0/2][66/782] Loss_D: 0.7589 Loss_G: 6.6117\n","[0/2][67/782] Loss_D: 0.3157 Loss_G: 5.5958\n","[0/2][68/782] Loss_D: 0.3357 Loss_G: 4.1901\n","[0/2][69/782] Loss_D: 0.3688 Loss_G: 4.8167\n","[0/2][70/782] Loss_D: 0.4430 Loss_G: 4.1577\n","[0/2][71/782] Loss_D: 0.4664 Loss_G: 6.3342\n","[0/2][72/782] Loss_D: 0.7891 Loss_G: 2.2631\n","[0/2][73/782] Loss_D: 1.2928 Loss_G: 10.1588\n","[0/2][74/782] Loss_D: 1.1504 Loss_G: 7.3351\n","[0/2][75/782] Loss_D: 0.2767 Loss_G: 3.5948\n","[0/2][76/782] Loss_D: 0.7537 Loss_G: 5.1114\n","[0/2][77/782] Loss_D: 0.2556 Loss_G: 5.5759\n","[0/2][78/782] Loss_D: 0.4943 Loss_G: 4.0510\n","[0/2][79/782] Loss_D: 0.4923 Loss_G: 4.9957\n","[0/2][80/782] Loss_D: 0.3620 Loss_G: 4.6336\n","[0/2][81/782] Loss_D: 0.3443 Loss_G: 4.0010\n","[0/2][82/782] Loss_D: 0.4015 Loss_G: 6.3216\n","[0/2][83/782] Loss_D: 0.2819 Loss_G: 4.6292\n","[0/2][84/782] Loss_D: 0.4833 Loss_G: 3.7149\n","[0/2][85/782] Loss_D: 0.6374 Loss_G: 8.3231\n","[0/2][86/782] Loss_D: 1.3600 Loss_G: 3.0012\n","[0/2][87/782] Loss_D: 0.8442 Loss_G: 7.3803\n","[0/2][88/782] Loss_D: 0.5654 Loss_G: 5.2274\n","[0/2][89/782] Loss_D: 0.2946 Loss_G: 4.2652\n","[0/2][90/782] Loss_D: 0.5030 Loss_G: 6.4730\n","[0/2][91/782] Loss_D: 0.4052 Loss_G: 4.8381\n","[0/2][92/782] Loss_D: 0.3043 Loss_G: 5.5762\n","[0/2][93/782] Loss_D: 0.3228 Loss_G: 4.5320\n","[0/2][94/782] Loss_D: 0.3119 Loss_G: 6.4883\n","[0/2][95/782] Loss_D: 0.3987 Loss_G: 4.0658\n","[0/2][96/782] Loss_D: 0.3333 Loss_G: 5.5755\n","[0/2][97/782] Loss_D: 0.1717 Loss_G: 5.5690\n","[0/2][98/782] Loss_D: 0.2375 Loss_G: 4.8535\n","[0/2][99/782] Loss_D: 0.3026 Loss_G: 5.1703\n","[0/2][100/782] Loss_D: 0.1840 Loss_G: 4.9238\n","[0/2][101/782] Loss_D: 0.2556 Loss_G: 5.7307\n","[0/2][102/782] Loss_D: 0.2145 Loss_G: 4.4677\n","[0/2][103/782] Loss_D: 0.2788 Loss_G: 4.8276\n","[0/2][104/782] Loss_D: 0.2644 Loss_G: 5.6218\n","[0/2][105/782] Loss_D: 0.3014 Loss_G: 4.5178\n","[0/2][106/782] Loss_D: 0.2219 Loss_G: 5.7612\n","[0/2][107/782] Loss_D: 0.2069 Loss_G: 5.0211\n","[0/2][108/782] Loss_D: 0.1747 Loss_G: 5.0117\n","[0/2][109/782] Loss_D: 0.1885 Loss_G: 6.2187\n","[0/2][110/782] Loss_D: 0.3278 Loss_G: 4.0609\n","[0/2][111/782] Loss_D: 0.3076 Loss_G: 6.5101\n","[0/2][112/782] Loss_D: 0.1007 Loss_G: 6.1891\n","[0/2][113/782] Loss_D: 0.1552 Loss_G: 4.9587\n","[0/2][114/782] Loss_D: 0.1731 Loss_G: 5.3697\n","[0/2][115/782] Loss_D: 0.2184 Loss_G: 5.7808\n","[0/2][116/782] Loss_D: 0.1438 Loss_G: 5.2317\n","[0/2][117/782] Loss_D: 0.2127 Loss_G: 4.9378\n","[0/2][118/782] Loss_D: 0.1734 Loss_G: 5.7442\n","[0/2][119/782] Loss_D: 0.1803 Loss_G: 4.9631\n","[0/2][120/782] Loss_D: 0.1695 Loss_G: 6.0908\n","[0/2][121/782] Loss_D: 0.0765 Loss_G: 5.9912\n","[0/2][122/782] Loss_D: 0.1429 Loss_G: 5.0142\n","[0/2][123/782] Loss_D: 0.3300 Loss_G: 5.6283\n","[0/2][124/782] Loss_D: 0.1612 Loss_G: 5.9110\n","[0/2][125/782] Loss_D: 0.1745 Loss_G: 5.7454\n","[0/2][126/782] Loss_D: 0.2680 Loss_G: 6.7776\n","[0/2][127/782] Loss_D: 0.1367 Loss_G: 6.1861\n","[0/2][128/782] Loss_D: 0.1180 Loss_G: 5.7332\n","[0/2][129/782] Loss_D: 0.1742 Loss_G: 7.5543\n","[0/2][130/782] Loss_D: 0.1988 Loss_G: 5.9043\n","[0/2][131/782] Loss_D: 0.1228 Loss_G: 6.1268\n","[0/2][132/782] Loss_D: 0.1144 Loss_G: 6.5903\n","[0/2][133/782] Loss_D: 0.1520 Loss_G: 5.0716\n","[0/2][134/782] Loss_D: 0.2609 Loss_G: 10.9274\n","[0/2][135/782] Loss_D: 0.2573 Loss_G: 8.9525\n","[0/2][136/782] Loss_D: 0.1993 Loss_G: 4.9697\n","[0/2][137/782] Loss_D: 0.4955 Loss_G: 9.8141\n","[0/2][138/782] Loss_D: 0.2478 Loss_G: 7.5686\n","[0/2][139/782] Loss_D: 0.1236 Loss_G: 6.4067\n","[0/2][140/782] Loss_D: 0.2695 Loss_G: 8.4874\n","[0/2][141/782] Loss_D: 0.1475 Loss_G: 7.1279\n","[0/2][142/782] Loss_D: 0.2356 Loss_G: 5.7440\n","[0/2][143/782] Loss_D: 0.3732 Loss_G: 10.4006\n","[0/2][144/782] Loss_D: 0.3319 Loss_G: 7.9327\n","[0/2][145/782] Loss_D: 0.1219 Loss_G: 5.1670\n","[0/2][146/782] Loss_D: 0.2918 Loss_G: 10.0196\n","[0/2][147/782] Loss_D: 0.0577 Loss_G: 10.0010\n","[0/2][148/782] Loss_D: 0.2516 Loss_G: 6.5115\n","[0/2][149/782] Loss_D: 0.0999 Loss_G: 4.8267\n","[0/2][150/782] Loss_D: 0.3155 Loss_G: 9.7485\n","[0/2][151/782] Loss_D: 0.1628 Loss_G: 8.8583\n","[0/2][152/782] Loss_D: 0.2004 Loss_G: 5.4081\n","[0/2][153/782] Loss_D: 0.1500 Loss_G: 6.3302\n","[0/2][154/782] Loss_D: 0.0612 Loss_G: 6.8359\n","[0/2][155/782] Loss_D: 0.1203 Loss_G: 5.8037\n","[0/2][156/782] Loss_D: 0.0884 Loss_G: 6.3887\n","[0/2][157/782] Loss_D: 0.1421 Loss_G: 6.0003\n","[0/2][158/782] Loss_D: 0.1522 Loss_G: 5.0286\n","[0/2][159/782] Loss_D: 0.1317 Loss_G: 6.3644\n","[0/2][160/782] Loss_D: 0.1861 Loss_G: 5.0035\n","[0/2][161/782] Loss_D: 0.1935 Loss_G: 7.1466\n","[0/2][162/782] Loss_D: 0.2309 Loss_G: 5.8329\n","[0/2][163/782] Loss_D: 0.4057 Loss_G: 5.6216\n","[0/2][164/782] Loss_D: 0.3161 Loss_G: 10.6064\n","[0/2][165/782] Loss_D: 0.8438 Loss_G: 5.2822\n","[0/2][166/782] Loss_D: 0.6551 Loss_G: 16.5349\n","[0/2][167/782] Loss_D: 4.8801 Loss_G: 8.0720\n","[0/2][168/782] Loss_D: 0.3782 Loss_G: 9.4824\n","[0/2][169/782] Loss_D: 0.0363 Loss_G: 7.9584\n","[0/2][170/782] Loss_D: 0.1453 Loss_G: 5.7514\n","[0/2][171/782] Loss_D: 0.5245 Loss_G: 11.8128\n","[0/2][172/782] Loss_D: 0.4820 Loss_G: 7.9273\n","[0/2][173/782] Loss_D: 0.1600 Loss_G: 4.3114\n","[0/2][174/782] Loss_D: 1.1390 Loss_G: 13.5687\n","[0/2][175/782] Loss_D: 1.6024 Loss_G: 8.7354\n","[0/2][176/782] Loss_D: 0.7938 Loss_G: 7.6598\n","[0/2][177/782] Loss_D: 0.7849 Loss_G: 9.3078\n","[0/2][178/782] Loss_D: 0.1778 Loss_G: 7.9013\n","[0/2][179/782] Loss_D: 0.4086 Loss_G: 8.1065\n","[0/2][180/782] Loss_D: 0.2445 Loss_G: 7.2403\n","[0/2][181/782] Loss_D: 0.2140 Loss_G: 5.9316\n","[0/2][182/782] Loss_D: 0.2247 Loss_G: 6.5337\n","[0/2][183/782] Loss_D: 0.2193 Loss_G: 5.1146\n","[0/2][184/782] Loss_D: 0.2661 Loss_G: 6.5872\n","[0/2][185/782] Loss_D: 0.2280 Loss_G: 5.0905\n","[0/2][186/782] Loss_D: 0.1224 Loss_G: 4.7311\n","[0/2][187/782] Loss_D: 0.0676 Loss_G: 5.1545\n","[0/2][188/782] Loss_D: 0.0613 Loss_G: 5.1640\n","[0/2][189/782] Loss_D: 0.0537 Loss_G: 5.0552\n","[0/2][190/782] Loss_D: 0.0829 Loss_G: 4.7035\n","[0/2][191/782] Loss_D: 0.0943 Loss_G: 5.1337\n","[0/2][192/782] Loss_D: 0.0774 Loss_G: 5.2005\n","[0/2][193/782] Loss_D: 0.1087 Loss_G: 4.5205\n","[0/2][194/782] Loss_D: 0.1705 Loss_G: 5.0594\n","[0/2][195/782] Loss_D: 0.1181 Loss_G: 4.6868\n","[0/2][196/782] Loss_D: 0.1791 Loss_G: 4.4779\n","[0/2][197/782] Loss_D: 0.0978 Loss_G: 5.0187\n","[0/2][198/782] Loss_D: 0.0512 Loss_G: 5.2349\n","[0/2][199/782] Loss_D: 0.1959 Loss_G: 4.8335\n","[0/2][200/782] Loss_D: 0.1381 Loss_G: 4.7727\n","[0/2][201/782] Loss_D: 0.1559 Loss_G: 6.2087\n","[0/2][202/782] Loss_D: 0.1773 Loss_G: 4.7275\n","[0/2][203/782] Loss_D: 0.1775 Loss_G: 5.6372\n","[0/2][204/782] Loss_D: 0.0840 Loss_G: 5.7192\n","[0/2][205/782] Loss_D: 0.0929 Loss_G: 5.3222\n","[0/2][206/782] Loss_D: 0.0835 Loss_G: 5.3418\n","[0/2][207/782] Loss_D: 0.0900 Loss_G: 5.0836\n","[0/2][208/782] Loss_D: 0.1850 Loss_G: 5.2175\n","[0/2][209/782] Loss_D: 0.1433 Loss_G: 5.9645\n","[0/2][210/782] Loss_D: 0.2934 Loss_G: 4.3015\n","[0/2][211/782] Loss_D: 0.1895 Loss_G: 7.5230\n","[0/2][212/782] Loss_D: 0.1664 Loss_G: 6.5014\n","[0/2][213/782] Loss_D: 0.1581 Loss_G: 7.8829\n","[0/2][214/782] Loss_D: 0.1656 Loss_G: 7.1263\n","[0/2][215/782] Loss_D: 0.6206 Loss_G: 15.5045\n","[0/2][216/782] Loss_D: 0.9744 Loss_G: 11.5806\n","[0/2][217/782] Loss_D: 0.0823 Loss_G: 5.7876\n","[0/2][218/782] Loss_D: 1.1547 Loss_G: 20.4174\n","[0/2][219/782] Loss_D: 2.5883 Loss_G: 17.4123\n","[0/2][220/782] Loss_D: 0.4598 Loss_G: 11.1055\n","[0/2][221/782] Loss_D: 0.0955 Loss_G: 6.2019\n","[0/2][222/782] Loss_D: 1.0840 Loss_G: 15.1038\n","[0/2][223/782] Loss_D: 0.5913 Loss_G: 14.3598\n","[0/2][224/782] Loss_D: 0.5545 Loss_G: 10.7607\n","[0/2][225/782] Loss_D: 0.1143 Loss_G: 6.9229\n","[0/2][226/782] Loss_D: 0.3163 Loss_G: 7.4574\n","[0/2][227/782] Loss_D: 0.1050 Loss_G: 8.2478\n","[0/2][228/782] Loss_D: 0.0921 Loss_G: 7.6768\n","[0/2][229/782] Loss_D: 0.1739 Loss_G: 6.0664\n","[0/2][230/782] Loss_D: 0.5239 Loss_G: 12.8174\n","[0/2][231/782] Loss_D: 0.5159 Loss_G: 9.6494\n","[0/2][232/782] Loss_D: 0.0651 Loss_G: 5.9040\n","[0/2][233/782] Loss_D: 0.4309 Loss_G: 7.4417\n","[0/2][234/782] Loss_D: 0.0594 Loss_G: 6.9453\n","[0/2][235/782] Loss_D: 0.1296 Loss_G: 5.7135\n","[0/2][236/782] Loss_D: 0.1267 Loss_G: 5.0346\n","[0/2][237/782] Loss_D: 0.2525 Loss_G: 6.3889\n","[0/2][238/782] Loss_D: 0.1749 Loss_G: 5.5206\n","[0/2][239/782] Loss_D: 0.1640 Loss_G: 4.4482\n","[0/2][240/782] Loss_D: 0.2910 Loss_G: 5.2878\n","[0/2][241/782] Loss_D: 0.3024 Loss_G: 4.2827\n","[0/2][242/782] Loss_D: 0.1322 Loss_G: 5.4957\n","[0/2][243/782] Loss_D: 0.0881 Loss_G: 5.8993\n","[0/2][244/782] Loss_D: 0.2213 Loss_G: 4.2759\n","[0/2][245/782] Loss_D: 0.2111 Loss_G: 5.9386\n","[0/2][246/782] Loss_D: 0.1424 Loss_G: 5.6750\n","[0/2][247/782] Loss_D: 0.0925 Loss_G: 5.2625\n","[0/2][248/782] Loss_D: 0.2200 Loss_G: 4.0566\n","[0/2][249/782] Loss_D: 0.3678 Loss_G: 7.5682\n","[0/2][250/782] Loss_D: 0.1558 Loss_G: 7.0729\n","[0/2][251/782] Loss_D: 0.1288 Loss_G: 4.8977\n","[0/2][252/782] Loss_D: 0.0778 Loss_G: 4.4308\n","[0/2][253/782] Loss_D: 0.2524 Loss_G: 5.2069\n","[0/2][254/782] Loss_D: 0.1180 Loss_G: 5.1341\n","[0/2][255/782] Loss_D: 0.1389 Loss_G: 4.9662\n","[0/2][256/782] Loss_D: 0.1985 Loss_G: 5.3395\n","[0/2][257/782] Loss_D: 0.3121 Loss_G: 5.4916\n","[0/2][258/782] Loss_D: 0.6310 Loss_G: 2.3303\n","[0/2][259/782] Loss_D: 0.8357 Loss_G: 10.9603\n","[0/2][260/782] Loss_D: 0.7734 Loss_G: 8.9876\n","[0/2][261/782] Loss_D: 0.0674 Loss_G: 6.2367\n","[0/2][262/782] Loss_D: 0.0618 Loss_G: 4.0188\n","[0/2][263/782] Loss_D: 0.2223 Loss_G: 4.5731\n","[0/2][264/782] Loss_D: 0.1833 Loss_G: 5.4292\n","[0/2][265/782] Loss_D: 0.1410 Loss_G: 5.1825\n","[0/2][266/782] Loss_D: 0.2863 Loss_G: 4.0243\n","[0/2][267/782] Loss_D: 0.3560 Loss_G: 4.6714\n","[0/2][268/782] Loss_D: 0.5140 Loss_G: 4.0906\n","[0/2][269/782] Loss_D: 0.2497 Loss_G: 5.6248\n","[0/2][270/782] Loss_D: 0.2318 Loss_G: 4.2670\n","[0/2][271/782] Loss_D: 0.4537 Loss_G: 5.4830\n","[0/2][272/782] Loss_D: 0.5888 Loss_G: 3.0041\n","[0/2][273/782] Loss_D: 0.6789 Loss_G: 7.2385\n","[0/2][274/782] Loss_D: 2.4054 Loss_G: 1.1355\n","[0/2][275/782] Loss_D: 1.3091 Loss_G: 7.9716\n","[0/2][276/782] Loss_D: 1.0479 Loss_G: 5.6795\n","[0/2][277/782] Loss_D: 0.1485 Loss_G: 4.0444\n","[0/2][278/782] Loss_D: 0.2637 Loss_G: 4.4353\n","[0/2][279/782] Loss_D: 0.2274 Loss_G: 4.9331\n","[0/2][280/782] Loss_D: 0.2162 Loss_G: 5.2506\n","[0/2][281/782] Loss_D: 0.2318 Loss_G: 4.2940\n","[0/2][282/782] Loss_D: 0.3570 Loss_G: 3.5384\n","[0/2][283/782] Loss_D: 0.7909 Loss_G: 5.9603\n","[0/2][284/782] Loss_D: 0.5007 Loss_G: 4.7051\n","[0/2][285/782] Loss_D: 0.2831 Loss_G: 3.9794\n","[0/2][286/782] Loss_D: 0.3733 Loss_G: 3.8476\n","[0/2][287/782] Loss_D: 0.4964 Loss_G: 5.1553\n","[0/2][288/782] Loss_D: 0.4099 Loss_G: 3.6732\n","[0/2][289/782] Loss_D: 0.4271 Loss_G: 3.6253\n","[0/2][290/782] Loss_D: 0.2695 Loss_G: 4.3516\n","[0/2][291/782] Loss_D: 0.2385 Loss_G: 4.0454\n","[0/2][292/782] Loss_D: 0.2800 Loss_G: 4.9020\n","[0/2][293/782] Loss_D: 0.2577 Loss_G: 3.8838\n","[0/2][294/782] Loss_D: 0.2236 Loss_G: 3.5486\n","[0/2][295/782] Loss_D: 0.5187 Loss_G: 6.5990\n","[0/2][296/782] Loss_D: 0.7086 Loss_G: 1.9022\n","[0/2][297/782] Loss_D: 0.8679 Loss_G: 7.2135\n","[0/2][298/782] Loss_D: 0.9911 Loss_G: 2.9397\n","[0/2][299/782] Loss_D: 0.6132 Loss_G: 4.1032\n","[0/2][300/782] Loss_D: 0.2413 Loss_G: 5.1141\n","[0/2][301/782] Loss_D: 0.3830 Loss_G: 3.5971\n","[0/2][302/782] Loss_D: 0.3049 Loss_G: 3.5044\n","[0/2][303/782] Loss_D: 0.4166 Loss_G: 5.1383\n","[0/2][304/782] Loss_D: 0.7085 Loss_G: 2.2221\n","[0/2][305/782] Loss_D: 0.8464 Loss_G: 6.7643\n","[0/2][306/782] Loss_D: 1.5448 Loss_G: 1.6532\n","[0/2][307/782] Loss_D: 1.2439 Loss_G: 6.4856\n","[0/2][308/782] Loss_D: 0.7681 Loss_G: 4.5509\n","[0/2][309/782] Loss_D: 0.3704 Loss_G: 3.7185\n","[0/2][310/782] Loss_D: 0.5753 Loss_G: 3.6490\n","[0/2][311/782] Loss_D: 0.5432 Loss_G: 5.3580\n","[0/2][312/782] Loss_D: 0.4748 Loss_G: 3.3727\n","[0/2][313/782] Loss_D: 0.6321 Loss_G: 4.7272\n","[0/2][314/782] Loss_D: 0.3532 Loss_G: 4.1328\n","[0/2][315/782] Loss_D: 0.4966 Loss_G: 3.3162\n","[0/2][316/782] Loss_D: 0.5322 Loss_G: 4.4663\n","[0/2][317/782] Loss_D: 0.5014 Loss_G: 3.0023\n","[0/2][318/782] Loss_D: 0.5613 Loss_G: 4.9769\n","[0/2][319/782] Loss_D: 0.7503 Loss_G: 1.7386\n","[0/2][320/782] Loss_D: 1.4272 Loss_G: 8.1799\n","[0/2][321/782] Loss_D: 1.6052 Loss_G: 2.7158\n","[0/2][322/782] Loss_D: 0.5578 Loss_G: 2.9553\n","[0/2][323/782] Loss_D: 0.7339 Loss_G: 6.5425\n","[0/2][324/782] Loss_D: 1.3677 Loss_G: 1.3491\n","[0/2][325/782] Loss_D: 1.2788 Loss_G: 7.2475\n","[0/2][326/782] Loss_D: 1.0514 Loss_G: 3.9129\n","[0/2][327/782] Loss_D: 0.4165 Loss_G: 2.6660\n","[0/2][328/782] Loss_D: 0.5352 Loss_G: 4.4753\n","[0/2][329/782] Loss_D: 0.4453 Loss_G: 3.7719\n","[0/2][330/782] Loss_D: 0.4284 Loss_G: 3.6676\n","[0/2][331/782] Loss_D: 0.5166 Loss_G: 4.3689\n","[0/2][332/782] Loss_D: 0.8034 Loss_G: 2.1322\n","[0/2][333/782] Loss_D: 1.1711 Loss_G: 8.0593\n","[0/2][334/782] Loss_D: 1.9999 Loss_G: 2.1027\n","[0/2][335/782] Loss_D: 0.8398 Loss_G: 6.2963\n","[0/2][336/782] Loss_D: 0.9036 Loss_G: 2.0349\n","[0/2][337/782] Loss_D: 0.8862 Loss_G: 6.0770\n","[0/2][338/782] Loss_D: 0.8429 Loss_G: 3.1521\n","[0/2][339/782] Loss_D: 0.6667 Loss_G: 2.9817\n","[0/2][340/782] Loss_D: 0.6104 Loss_G: 4.2004\n","[0/2][341/782] Loss_D: 0.3902 Loss_G: 3.4222\n","[0/2][342/782] Loss_D: 0.3144 Loss_G: 3.4212\n","[0/2][343/782] Loss_D: 0.4517 Loss_G: 4.7735\n","[0/2][344/782] Loss_D: 0.5891 Loss_G: 2.6850\n","[0/2][345/782] Loss_D: 0.8718 Loss_G: 6.1616\n","[0/2][346/782] Loss_D: 0.5944 Loss_G: 3.2170\n","[0/2][347/782] Loss_D: 0.7037 Loss_G: 6.2510\n","[0/2][348/782] Loss_D: 0.6373 Loss_G: 2.7406\n","[0/2][349/782] Loss_D: 0.7123 Loss_G: 5.5795\n","[0/2][350/782] Loss_D: 0.7289 Loss_G: 2.9083\n","[0/2][351/782] Loss_D: 0.7787 Loss_G: 5.4726\n","[0/2][352/782] Loss_D: 0.4610 Loss_G: 4.2904\n","[0/2][353/782] Loss_D: 0.2346 Loss_G: 3.1251\n","[0/2][354/782] Loss_D: 0.3440 Loss_G: 4.0427\n","[0/2][355/782] Loss_D: 0.2987 Loss_G: 4.3963\n","[0/2][356/782] Loss_D: 0.2128 Loss_G: 4.0042\n","[0/2][357/782] Loss_D: 0.2795 Loss_G: 3.9345\n","[0/2][358/782] Loss_D: 0.4429 Loss_G: 2.5236\n","[0/2][359/782] Loss_D: 0.8780 Loss_G: 7.9392\n","[0/2][360/782] Loss_D: 0.9479 Loss_G: 2.1193\n","[0/2][361/782] Loss_D: 1.3793 Loss_G: 9.3662\n","[0/2][362/782] Loss_D: 1.7896 Loss_G: 0.8225\n","[0/2][363/782] Loss_D: 2.4706 Loss_G: 9.8900\n","[0/2][364/782] Loss_D: 3.0100 Loss_G: 3.2189\n","[0/2][365/782] Loss_D: 0.8440 Loss_G: 3.1918\n","[0/2][366/782] Loss_D: 1.2343 Loss_G: 3.1715\n","[0/2][367/782] Loss_D: 0.6595 Loss_G: 4.9942\n","[0/2][368/782] Loss_D: 0.9981 Loss_G: 1.8158\n","[0/2][369/782] Loss_D: 1.0326 Loss_G: 5.2461\n","[0/2][370/782] Loss_D: 0.4884 Loss_G: 3.6564\n","[0/2][371/782] Loss_D: 0.4955 Loss_G: 4.1932\n","[0/2][372/782] Loss_D: 0.8181 Loss_G: 2.4280\n","[0/2][373/782] Loss_D: 0.9784 Loss_G: 6.9870\n","[0/2][374/782] Loss_D: 1.1678 Loss_G: 2.6566\n","[0/2][375/782] Loss_D: 0.6115 Loss_G: 6.0577\n","[0/2][376/782] Loss_D: 0.2995 Loss_G: 4.7754\n","[0/2][377/782] Loss_D: 0.3449 Loss_G: 2.5233\n","[0/2][378/782] Loss_D: 0.6317 Loss_G: 5.5802\n","[0/2][379/782] Loss_D: 0.4946 Loss_G: 3.7825\n","[0/2][380/782] Loss_D: 0.1947 Loss_G: 3.3280\n","[0/2][381/782] Loss_D: 0.3199 Loss_G: 4.8263\n","[0/2][382/782] Loss_D: 0.4482 Loss_G: 3.1842\n","[0/2][383/782] Loss_D: 0.3925 Loss_G: 3.4958\n","[0/2][384/782] Loss_D: 0.3465 Loss_G: 4.8991\n","[0/2][385/782] Loss_D: 0.1309 Loss_G: 4.9308\n","[0/2][386/782] Loss_D: 0.4094 Loss_G: 2.3397\n","[0/2][387/782] Loss_D: 0.7323 Loss_G: 6.9162\n","[0/2][388/782] Loss_D: 1.0196 Loss_G: 4.6423\n","[0/2][389/782] Loss_D: 0.1875 Loss_G: 3.5648\n","[0/2][390/782] Loss_D: 0.2613 Loss_G: 4.8674\n","[0/2][391/782] Loss_D: 0.1761 Loss_G: 5.0632\n","[0/2][392/782] Loss_D: 0.1772 Loss_G: 4.3209\n","[0/2][393/782] Loss_D: 0.2666 Loss_G: 4.5256\n","[0/2][394/782] Loss_D: 0.2313 Loss_G: 4.7670\n","[0/2][395/782] Loss_D: 0.2985 Loss_G: 4.7352\n","[0/2][396/782] Loss_D: 0.4011 Loss_G: 4.8219\n","[0/2][397/782] Loss_D: 0.3674 Loss_G: 7.2130\n","[0/2][398/782] Loss_D: 0.6733 Loss_G: 2.3702\n","[0/2][399/782] Loss_D: 1.6070 Loss_G: 12.7376\n","[0/2][400/782] Loss_D: 2.5116 Loss_G: 8.0834\n","[0/2][401/782] Loss_D: 0.6146 Loss_G: 2.9984\n","[0/2][402/782] Loss_D: 0.7075 Loss_G: 5.1355\n","[0/2][403/782] Loss_D: 0.2314 Loss_G: 5.8127\n","[0/2][404/782] Loss_D: 0.1536 Loss_G: 4.8356\n","[0/2][405/782] Loss_D: 0.4138 Loss_G: 3.8118\n","[0/2][406/782] Loss_D: 0.2489 Loss_G: 5.3672\n","[0/2][407/782] Loss_D: 0.5273 Loss_G: 2.4365\n","[0/2][408/782] Loss_D: 1.1577 Loss_G: 9.9787\n","[0/2][409/782] Loss_D: 1.3288 Loss_G: 7.1979\n","[0/2][410/782] Loss_D: 0.1683 Loss_G: 3.3357\n","[0/2][411/782] Loss_D: 0.8971 Loss_G: 5.5059\n","[0/2][412/782] Loss_D: 0.4324 Loss_G: 4.5737\n","[0/2][413/782] Loss_D: 0.9880 Loss_G: 4.0191\n","[0/2][414/782] Loss_D: 0.5451 Loss_G: 4.7031\n","[0/2][415/782] Loss_D: 1.0874 Loss_G: 2.2962\n","[0/2][416/782] Loss_D: 0.7530 Loss_G: 6.7679\n","[0/2][417/782] Loss_D: 0.4795 Loss_G: 5.3306\n","[0/2][418/782] Loss_D: 0.2086 Loss_G: 4.8451\n","[0/2][419/782] Loss_D: 0.1824 Loss_G: 4.9434\n","[0/2][420/782] Loss_D: 0.3250 Loss_G: 3.6661\n","[0/2][421/782] Loss_D: 0.5660 Loss_G: 8.4996\n","[0/2][422/782] Loss_D: 1.0405 Loss_G: 3.7391\n","[0/2][423/782] Loss_D: 0.3116 Loss_G: 3.9515\n","[0/2][424/782] Loss_D: 0.6237 Loss_G: 9.1823\n","[0/2][425/782] Loss_D: 1.1082 Loss_G: 4.7732\n","[0/2][426/782] Loss_D: 0.5361 Loss_G: 6.9593\n","[0/2][427/782] Loss_D: 0.2616 Loss_G: 6.7318\n","[0/2][428/782] Loss_D: 0.6478 Loss_G: 4.0938\n","[0/2][429/782] Loss_D: 0.8337 Loss_G: 7.1857\n","[0/2][430/782] Loss_D: 0.2961 Loss_G: 5.5435\n","[0/2][431/782] Loss_D: 0.4386 Loss_G: 3.5259\n","[0/2][432/782] Loss_D: 0.8415 Loss_G: 7.2018\n","[0/2][433/782] Loss_D: 0.2603 Loss_G: 6.1937\n","[0/2][434/782] Loss_D: 0.4852 Loss_G: 3.0988\n","[0/2][435/782] Loss_D: 0.9142 Loss_G: 7.7258\n","[0/2][436/782] Loss_D: 0.3635 Loss_G: 6.4046\n","[0/2][437/782] Loss_D: 0.2319 Loss_G: 3.9067\n","[0/2][438/782] Loss_D: 0.5251 Loss_G: 5.5213\n","[0/2][439/782] Loss_D: 0.2390 Loss_G: 5.2109\n","[0/2][440/782] Loss_D: 0.4844 Loss_G: 3.5811\n","[0/2][441/782] Loss_D: 0.6934 Loss_G: 8.3862\n","[0/2][442/782] Loss_D: 0.7458 Loss_G: 5.7002\n","[0/2][443/782] Loss_D: 0.2415 Loss_G: 3.8200\n","[0/2][444/782] Loss_D: 0.3095 Loss_G: 4.8639\n","[0/2][445/782] Loss_D: 0.2655 Loss_G: 4.9815\n","[0/2][446/782] Loss_D: 0.3551 Loss_G: 3.8877\n","[0/2][447/782] Loss_D: 0.2333 Loss_G: 4.2651\n","[0/2][448/782] Loss_D: 0.1812 Loss_G: 4.5554\n","[0/2][449/782] Loss_D: 0.4191 Loss_G: 2.6176\n","[0/2][450/782] Loss_D: 0.3903 Loss_G: 4.3196\n","[0/2][451/782] Loss_D: 0.2956 Loss_G: 4.2463\n","[0/2][452/782] Loss_D: 0.2561 Loss_G: 3.8540\n","[0/2][453/782] Loss_D: 0.1648 Loss_G: 4.2586\n","[0/2][454/782] Loss_D: 0.2017 Loss_G: 3.7826\n","[0/2][455/782] Loss_D: 0.3757 Loss_G: 3.0687\n","[0/2][456/782] Loss_D: 0.4371 Loss_G: 6.2969\n","[0/2][457/782] Loss_D: 0.7498 Loss_G: 2.9229\n","[0/2][458/782] Loss_D: 0.5069 Loss_G: 5.9616\n","[0/2][459/782] Loss_D: 0.4426 Loss_G: 4.0277\n","[0/2][460/782] Loss_D: 0.2533 Loss_G: 3.4812\n","[0/2][461/782] Loss_D: 0.5654 Loss_G: 7.3590\n","[0/2][462/782] Loss_D: 0.3601 Loss_G: 5.4588\n","[0/2][463/782] Loss_D: 0.1880 Loss_G: 3.2607\n","[0/2][464/782] Loss_D: 0.3791 Loss_G: 6.2306\n","[0/2][465/782] Loss_D: 0.3239 Loss_G: 4.0894\n","[0/2][466/782] Loss_D: 0.5226 Loss_G: 6.1079\n","[0/2][467/782] Loss_D: 0.3809 Loss_G: 3.9150\n","[0/2][468/782] Loss_D: 0.4468 Loss_G: 4.7695\n","[0/2][469/782] Loss_D: 0.2851 Loss_G: 4.8540\n","[0/2][470/782] Loss_D: 0.3387 Loss_G: 3.4546\n","[0/2][471/782] Loss_D: 0.3647 Loss_G: 6.4692\n","[0/2][472/782] Loss_D: 0.4907 Loss_G: 2.7288\n","[0/2][473/782] Loss_D: 0.6476 Loss_G: 8.0258\n","[0/2][474/782] Loss_D: 0.3044 Loss_G: 6.4860\n","[0/2][475/782] Loss_D: 0.2954 Loss_G: 2.4074\n","[0/2][476/782] Loss_D: 1.1718 Loss_G: 9.3342\n","[0/2][477/782] Loss_D: 2.2612 Loss_G: 2.2150\n","[0/2][478/782] Loss_D: 1.2364 Loss_G: 7.6814\n","[0/2][479/782] Loss_D: 0.8223 Loss_G: 3.6387\n","[0/2][480/782] Loss_D: 0.6688 Loss_G: 6.8194\n","[0/2][481/782] Loss_D: 1.0312 Loss_G: 2.2453\n","[0/2][482/782] Loss_D: 1.2608 Loss_G: 9.4749\n","[0/2][483/782] Loss_D: 3.0705 Loss_G: 0.9238\n","[0/2][484/782] Loss_D: 2.3446 Loss_G: 8.2153\n","[0/2][485/782] Loss_D: 1.1136 Loss_G: 4.3858\n","[0/2][486/782] Loss_D: 0.6313 Loss_G: 2.2030\n","[0/2][487/782] Loss_D: 0.6763 Loss_G: 3.9996\n","[0/2][488/782] Loss_D: 0.5595 Loss_G: 3.6283\n","[0/2][489/782] Loss_D: 0.4000 Loss_G: 3.1631\n","[0/2][490/782] Loss_D: 0.5435 Loss_G: 3.9492\n","[0/2][491/782] Loss_D: 0.6594 Loss_G: 4.2926\n","[0/2][492/782] Loss_D: 1.2068 Loss_G: 0.2794\n","[0/2][493/782] Loss_D: 2.0796 Loss_G: 10.0807\n","[0/2][494/782] Loss_D: 1.2856 Loss_G: 8.5622\n","[0/2][495/782] Loss_D: 0.7586 Loss_G: 4.0737\n","[0/2][496/782] Loss_D: 0.4356 Loss_G: 2.9860\n","[0/2][497/782] Loss_D: 0.5990 Loss_G: 5.1854\n","[0/2][498/782] Loss_D: 0.1148 Loss_G: 5.5737\n","[0/2][499/782] Loss_D: 0.3260 Loss_G: 3.8755\n","[0/2][500/782] Loss_D: 0.3816 Loss_G: 3.3671\n","[0/2][501/782] Loss_D: 0.3778 Loss_G: 3.9657\n","[0/2][502/782] Loss_D: 0.4853 Loss_G: 5.5063\n","[0/2][503/782] Loss_D: 0.8872 Loss_G: 1.7039\n","[0/2][504/782] Loss_D: 0.6793 Loss_G: 6.5562\n","[0/2][505/782] Loss_D: 0.4718 Loss_G: 4.5132\n","[0/2][506/782] Loss_D: 0.2707 Loss_G: 4.1688\n","[0/2][507/782] Loss_D: 0.2116 Loss_G: 4.8535\n","[0/2][508/782] Loss_D: 0.3028 Loss_G: 5.5864\n","[0/2][509/782] Loss_D: 0.5702 Loss_G: 2.8769\n","[0/2][510/782] Loss_D: 0.5371 Loss_G: 6.3949\n","[0/2][511/782] Loss_D: 0.1974 Loss_G: 5.9443\n","[0/2][512/782] Loss_D: 0.6302 Loss_G: 2.2461\n","[0/2][513/782] Loss_D: 1.4301 Loss_G: 10.8390\n","[0/2][514/782] Loss_D: 1.5739 Loss_G: 7.8721\n","[0/2][515/782] Loss_D: 0.2446 Loss_G: 4.1507\n","[0/2][516/782] Loss_D: 1.0322 Loss_G: 6.8700\n","[0/2][517/782] Loss_D: 0.2545 Loss_G: 5.8967\n","[0/2][518/782] Loss_D: 0.3809 Loss_G: 4.6930\n","[0/2][519/782] Loss_D: 0.5962 Loss_G: 5.7910\n","[0/2][520/782] Loss_D: 0.2824 Loss_G: 5.3701\n","[0/2][521/782] Loss_D: 0.5630 Loss_G: 2.5098\n","[0/2][522/782] Loss_D: 1.0328 Loss_G: 9.0384\n","[0/2][523/782] Loss_D: 1.2846 Loss_G: 3.9092\n","[0/2][524/782] Loss_D: 0.3084 Loss_G: 3.2659\n","[0/2][525/782] Loss_D: 0.4766 Loss_G: 6.2706\n","[0/2][526/782] Loss_D: 0.6934 Loss_G: 3.1519\n","[0/2][527/782] Loss_D: 1.3650 Loss_G: 3.9058\n","[0/2][528/782] Loss_D: 0.4522 Loss_G: 5.1081\n","[0/2][529/782] Loss_D: 0.7655 Loss_G: 1.4452\n","[0/2][530/782] Loss_D: 0.7604 Loss_G: 6.0309\n","[0/2][531/782] Loss_D: 0.3557 Loss_G: 5.4523\n","[0/2][532/782] Loss_D: 0.4561 Loss_G: 2.2790\n","[0/2][533/782] Loss_D: 0.6633 Loss_G: 5.0160\n","[0/2][534/782] Loss_D: 0.3908 Loss_G: 3.8491\n","[0/2][535/782] Loss_D: 0.4087 Loss_G: 2.4479\n","[0/2][536/782] Loss_D: 0.6723 Loss_G: 4.5336\n","[0/2][537/782] Loss_D: 0.5108 Loss_G: 3.5612\n","[0/2][538/782] Loss_D: 0.7388 Loss_G: 0.9080\n","[0/2][539/782] Loss_D: 1.5141 Loss_G: 8.4314\n","[0/2][540/782] Loss_D: 1.8157 Loss_G: 3.3511\n","[0/2][541/782] Loss_D: 0.2107 Loss_G: 2.0834\n","[0/2][542/782] Loss_D: 0.7944 Loss_G: 6.4241\n","[0/2][543/782] Loss_D: 0.4581 Loss_G: 4.6660\n","[0/2][544/782] Loss_D: 0.2692 Loss_G: 2.8714\n","[0/2][545/782] Loss_D: 0.4786 Loss_G: 4.4180\n","[0/2][546/782] Loss_D: 0.5076 Loss_G: 3.8350\n","[0/2][547/782] Loss_D: 0.4249 Loss_G: 3.3069\n","[0/2][548/782] Loss_D: 0.7971 Loss_G: 6.1886\n","[0/2][549/782] Loss_D: 1.2352 Loss_G: 1.8849\n","[0/2][550/782] Loss_D: 0.9891 Loss_G: 8.0009\n","[0/2][551/782] Loss_D: 0.3720 Loss_G: 7.3070\n","[0/2][552/782] Loss_D: 0.1781 Loss_G: 5.2395\n","[0/2][553/782] Loss_D: 0.1999 Loss_G: 3.3978\n","[0/2][554/782] Loss_D: 0.4306 Loss_G: 6.1627\n","[0/2][555/782] Loss_D: 0.2276 Loss_G: 5.0506\n","[0/2][556/782] Loss_D: 0.0989 Loss_G: 3.8936\n","[0/2][557/782] Loss_D: 0.4224 Loss_G: 4.8846\n","[0/2][558/782] Loss_D: 0.2579 Loss_G: 4.3937\n","[0/2][559/782] Loss_D: 0.3816 Loss_G: 2.8937\n","[0/2][560/782] Loss_D: 0.3781 Loss_G: 5.4459\n","[0/2][561/782] Loss_D: 0.2335 Loss_G: 4.7458\n","[0/2][562/782] Loss_D: 0.2803 Loss_G: 4.2017\n","[0/2][563/782] Loss_D: 0.2259 Loss_G: 4.3594\n","[0/2][564/782] Loss_D: 0.4542 Loss_G: 3.0217\n","[0/2][565/782] Loss_D: 0.8874 Loss_G: 8.2352\n","[0/2][566/782] Loss_D: 1.4411 Loss_G: 3.0127\n","[0/2][567/782] Loss_D: 0.6287 Loss_G: 7.5594\n","[0/2][568/782] Loss_D: 0.4590 Loss_G: 5.0051\n","[0/2][569/782] Loss_D: 0.1354 Loss_G: 3.9491\n","[0/2][570/782] Loss_D: 0.7462 Loss_G: 7.4185\n","[0/2][571/782] Loss_D: 0.8801 Loss_G: 4.5091\n","[0/2][572/782] Loss_D: 0.4284 Loss_G: 2.6215\n","[0/2][573/782] Loss_D: 0.3277 Loss_G: 5.1987\n","[0/2][574/782] Loss_D: 0.1535 Loss_G: 5.3584\n","[0/2][575/782] Loss_D: 0.2849 Loss_G: 3.3795\n","[0/2][576/782] Loss_D: 0.3210 Loss_G: 4.3234\n","[0/2][577/782] Loss_D: 0.2904 Loss_G: 3.9801\n","[0/2][578/782] Loss_D: 0.2463 Loss_G: 5.0524\n","[0/2][579/782] Loss_D: 0.2048 Loss_G: 4.3867\n","[0/2][580/782] Loss_D: 0.2007 Loss_G: 3.8675\n","[0/2][581/782] Loss_D: 0.5012 Loss_G: 7.0932\n","[0/2][582/782] Loss_D: 1.3993 Loss_G: 1.1647\n","[0/2][583/782] Loss_D: 1.9675 Loss_G: 8.4211\n","[0/2][584/782] Loss_D: 1.6913 Loss_G: 6.2162\n","[0/2][585/782] Loss_D: 0.1005 Loss_G: 3.6954\n","[0/2][586/782] Loss_D: 0.2160 Loss_G: 3.4140\n","[0/2][587/782] Loss_D: 0.4663 Loss_G: 4.2118\n","[0/2][588/782] Loss_D: 0.5166 Loss_G: 2.9852\n","[0/2][589/782] Loss_D: 0.7289 Loss_G: 3.7648\n","[0/2][590/782] Loss_D: 0.3902 Loss_G: 3.9791\n","[0/2][591/782] Loss_D: 0.3497 Loss_G: 3.2678\n","[0/2][592/782] Loss_D: 0.6372 Loss_G: 2.7775\n","[0/2][593/782] Loss_D: 1.2089 Loss_G: 9.7057\n","[0/2][594/782] Loss_D: 2.4254 Loss_G: 4.4758\n","[0/2][595/782] Loss_D: 0.3183 Loss_G: 1.8060\n","[0/2][596/782] Loss_D: 0.6528 Loss_G: 6.1055\n","[0/2][597/782] Loss_D: 0.5143 Loss_G: 3.7446\n","[0/2][598/782] Loss_D: 0.6152 Loss_G: 3.9561\n","[0/2][599/782] Loss_D: 0.3090 Loss_G: 4.6876\n","[0/2][600/782] Loss_D: 0.5754 Loss_G: 2.3839\n","[0/2][601/782] Loss_D: 0.8436 Loss_G: 6.1079\n","[0/2][602/782] Loss_D: 0.4109 Loss_G: 4.7797\n","[0/2][603/782] Loss_D: 0.5166 Loss_G: 1.9461\n","[0/2][604/782] Loss_D: 0.9393 Loss_G: 6.3925\n","[0/2][605/782] Loss_D: 0.1939 Loss_G: 4.7618\n","[0/2][606/782] Loss_D: 0.2514 Loss_G: 4.0343\n","[0/2][607/782] Loss_D: 0.4605 Loss_G: 2.5430\n","[0/2][608/782] Loss_D: 0.4879 Loss_G: 8.3455\n","[0/2][609/782] Loss_D: 1.3613 Loss_G: 2.9234\n","[0/2][610/782] Loss_D: 0.4913 Loss_G: 5.0704\n","[0/2][611/782] Loss_D: 0.1845 Loss_G: 6.1285\n","[0/2][612/782] Loss_D: 0.2452 Loss_G: 5.1183\n","[0/2][613/782] Loss_D: 0.5416 Loss_G: 7.3902\n","[0/2][614/782] Loss_D: 0.5003 Loss_G: 3.7917\n","[0/2][615/782] Loss_D: 0.8662 Loss_G: 8.4082\n","[0/2][616/782] Loss_D: 1.0846 Loss_G: 3.3410\n","[0/2][617/782] Loss_D: 1.0539 Loss_G: 6.6450\n","[0/2][618/782] Loss_D: 0.2015 Loss_G: 5.9794\n","[0/2][619/782] Loss_D: 0.5895 Loss_G: 2.1611\n","[0/2][620/782] Loss_D: 1.0258 Loss_G: 6.2282\n","[0/2][621/782] Loss_D: 0.6619 Loss_G: 3.4249\n","[0/2][622/782] Loss_D: 0.3052 Loss_G: 4.7851\n","[0/2][623/782] Loss_D: 0.3898 Loss_G: 4.1539\n","[0/2][624/782] Loss_D: 0.2771 Loss_G: 4.8991\n","[0/2][625/782] Loss_D: 0.4364 Loss_G: 4.8886\n","[0/2][626/782] Loss_D: 0.4352 Loss_G: 3.6328\n","[0/2][627/782] Loss_D: 0.4056 Loss_G: 7.5615\n","[0/2][628/782] Loss_D: 0.6282 Loss_G: 3.7949\n","[0/2][629/782] Loss_D: 0.5113 Loss_G: 4.8087\n","[0/2][630/782] Loss_D: 0.2230 Loss_G: 5.2316\n","[0/2][631/782] Loss_D: 0.6936 Loss_G: 5.1659\n","[0/2][632/782] Loss_D: 0.2005 Loss_G: 4.6889\n","[0/2][633/782] Loss_D: 0.7434 Loss_G: 9.1670\n","[0/2][634/782] Loss_D: 2.3046 Loss_G: 1.7226\n","[0/2][635/782] Loss_D: 1.3276 Loss_G: 5.7929\n","[0/2][636/782] Loss_D: 0.5020 Loss_G: 2.4864\n","[0/2][637/782] Loss_D: 0.7894 Loss_G: 5.0520\n","[0/2][638/782] Loss_D: 0.4716 Loss_G: 3.0719\n","[0/2][639/782] Loss_D: 0.3461 Loss_G: 4.2021\n","[0/2][640/782] Loss_D: 0.4326 Loss_G: 3.4329\n","[0/2][641/782] Loss_D: 0.4705 Loss_G: 4.5222\n","[0/2][642/782] Loss_D: 0.2502 Loss_G: 3.8212\n","[0/2][643/782] Loss_D: 0.3415 Loss_G: 2.5611\n","[0/2][644/782] Loss_D: 0.3708 Loss_G: 4.0937\n","[0/2][645/782] Loss_D: 0.3700 Loss_G: 5.3530\n","[0/2][646/782] Loss_D: 0.6381 Loss_G: 1.6828\n","[0/2][647/782] Loss_D: 0.7656 Loss_G: 6.3838\n","[0/2][648/782] Loss_D: 0.6419 Loss_G: 4.1915\n","[0/2][649/782] Loss_D: 0.2787 Loss_G: 2.8909\n","[0/2][650/782] Loss_D: 0.3035 Loss_G: 4.3341\n","[0/2][651/782] Loss_D: 0.2269 Loss_G: 4.2939\n","[0/2][652/782] Loss_D: 0.2781 Loss_G: 3.5240\n","[0/2][653/782] Loss_D: 0.4405 Loss_G: 2.9559\n","[0/2][654/782] Loss_D: 0.6173 Loss_G: 6.6200\n","[0/2][655/782] Loss_D: 0.8056 Loss_G: 2.3581\n","[0/2][656/782] Loss_D: 0.5031 Loss_G: 5.6376\n","[0/2][657/782] Loss_D: 0.2003 Loss_G: 4.8544\n","[0/2][658/782] Loss_D: 0.3194 Loss_G: 3.8609\n","[0/2][659/782] Loss_D: 0.4435 Loss_G: 3.0804\n","[0/2][660/782] Loss_D: 0.8212 Loss_G: 7.0007\n","[0/2][661/782] Loss_D: 1.3852 Loss_G: 0.3862\n","[0/2][662/782] Loss_D: 2.6319 Loss_G: 10.1057\n","[0/2][663/782] Loss_D: 3.5278 Loss_G: 0.9176\n","[0/2][664/782] Loss_D: 1.4343 Loss_G: 7.3990\n","[0/2][665/782] Loss_D: 1.2609 Loss_G: 3.8251\n","[0/2][666/782] Loss_D: 0.6370 Loss_G: 2.9787\n","[0/2][667/782] Loss_D: 0.8203 Loss_G: 5.1518\n","[0/2][668/782] Loss_D: 1.3829 Loss_G: 1.9222\n","[0/2][669/782] Loss_D: 0.7711 Loss_G: 4.2384\n","[0/2][670/782] Loss_D: 0.4153 Loss_G: 3.5713\n","[0/2][671/782] Loss_D: 0.5614 Loss_G: 1.9479\n","[0/2][672/782] Loss_D: 0.8911 Loss_G: 4.2186\n","[0/2][673/782] Loss_D: 0.4578 Loss_G: 3.7225\n","[0/2][674/782] Loss_D: 0.4181 Loss_G: 2.5644\n","[0/2][675/782] Loss_D: 0.8729 Loss_G: 4.9780\n","[0/2][676/782] Loss_D: 0.3742 Loss_G: 4.4829\n","[0/2][677/782] Loss_D: 0.2639 Loss_G: 3.2958\n","[0/2][678/782] Loss_D: 0.3058 Loss_G: 3.6865\n","[0/2][679/782] Loss_D: 0.5332 Loss_G: 3.0075\n","[0/2][680/782] Loss_D: 0.3299 Loss_G: 4.2273\n","[0/2][681/782] Loss_D: 0.3261 Loss_G: 4.3854\n","[0/2][682/782] Loss_D: 0.2415 Loss_G: 4.4596\n","[0/2][683/782] Loss_D: 0.2790 Loss_G: 3.3402\n","[0/2][684/782] Loss_D: 0.4839 Loss_G: 5.7865\n","[0/2][685/782] Loss_D: 0.9040 Loss_G: 2.3950\n","[0/2][686/782] Loss_D: 0.6113 Loss_G: 6.2550\n","[0/2][687/782] Loss_D: 0.2699 Loss_G: 5.2666\n","[0/2][688/782] Loss_D: 0.1010 Loss_G: 5.0390\n","[0/2][689/782] Loss_D: 0.1329 Loss_G: 4.5128\n","[0/2][690/782] Loss_D: 0.2189 Loss_G: 4.8996\n","[0/2][691/782] Loss_D: 0.2591 Loss_G: 4.7939\n","[0/2][692/782] Loss_D: 0.5893 Loss_G: 5.9337\n","[0/2][693/782] Loss_D: 0.4780 Loss_G: 3.1585\n","[0/2][694/782] Loss_D: 1.0244 Loss_G: 7.5993\n","[0/2][695/782] Loss_D: 0.8861 Loss_G: 4.5119\n","[0/2][696/782] Loss_D: 0.1966 Loss_G: 2.8756\n","[0/2][697/782] Loss_D: 0.5398 Loss_G: 5.9217\n","[0/2][698/782] Loss_D: 0.1771 Loss_G: 5.5888\n","[0/2][699/782] Loss_D: 0.2923 Loss_G: 3.5406\n","[0/2][700/782] Loss_D: 0.2244 Loss_G: 3.3305\n","[0/2][701/782] Loss_D: 0.2001 Loss_G: 4.2053\n","[0/2][702/782] Loss_D: 0.1890 Loss_G: 4.6150\n","[0/2][703/782] Loss_D: 0.2666 Loss_G: 3.7916\n","[0/2][704/782] Loss_D: 0.2964 Loss_G: 4.0137\n","[0/2][705/782] Loss_D: 0.3136 Loss_G: 3.7823\n","[0/2][706/782] Loss_D: 0.2704 Loss_G: 5.1350\n","[0/2][707/782] Loss_D: 0.5399 Loss_G: 2.3589\n","[0/2][708/782] Loss_D: 0.6930 Loss_G: 7.1574\n","[0/2][709/782] Loss_D: 0.9454 Loss_G: 3.0427\n","[0/2][710/782] Loss_D: 0.3536 Loss_G: 3.5890\n","[0/2][711/782] Loss_D: 0.3016 Loss_G: 5.7238\n","[0/2][712/782] Loss_D: 0.1002 Loss_G: 5.5365\n","[0/2][713/782] Loss_D: 0.2610 Loss_G: 3.9916\n","[0/2][714/782] Loss_D: 0.5050 Loss_G: 5.2340\n","[0/2][715/782] Loss_D: 0.5374 Loss_G: 3.6478\n","[0/2][716/782] Loss_D: 0.5068 Loss_G: 4.1454\n","[0/2][717/782] Loss_D: 0.3106 Loss_G: 5.1772\n","[0/2][718/782] Loss_D: 0.6892 Loss_G: 1.4388\n","[0/2][719/782] Loss_D: 1.2750 Loss_G: 7.5886\n","[0/2][720/782] Loss_D: 0.7281 Loss_G: 3.2555\n","[0/2][721/782] Loss_D: 0.8357 Loss_G: 8.1835\n","[0/2][722/782] Loss_D: 1.5073 Loss_G: 1.6803\n","[0/2][723/782] Loss_D: 1.7571 Loss_G: 7.8381\n","[0/2][724/782] Loss_D: 1.3473 Loss_G: 4.2704\n","[0/2][725/782] Loss_D: 0.3386 Loss_G: 3.8724\n","[0/2][726/782] Loss_D: 0.3252 Loss_G: 6.0875\n","[0/2][727/782] Loss_D: 0.4162 Loss_G: 3.7204\n","[0/2][728/782] Loss_D: 0.4321 Loss_G: 4.6996\n","[0/2][729/782] Loss_D: 0.3835 Loss_G: 4.5866\n","[0/2][730/782] Loss_D: 0.3287 Loss_G: 4.2710\n","[0/2][731/782] Loss_D: 0.4782 Loss_G: 3.0804\n","[0/2][732/782] Loss_D: 1.6726 Loss_G: 11.9367\n","[0/2][733/782] Loss_D: 4.0996 Loss_G: 4.0460\n","[0/2][734/782] Loss_D: 0.7136 Loss_G: 0.7542\n","[0/2][735/782] Loss_D: 1.8825 Loss_G: 7.6472\n","[0/2][736/782] Loss_D: 1.0880 Loss_G: 4.1168\n","[0/2][737/782] Loss_D: 0.2551 Loss_G: 2.8694\n","[0/2][738/782] Loss_D: 1.1490 Loss_G: 4.9754\n","[0/2][739/782] Loss_D: 0.8036 Loss_G: 3.5326\n","[0/2][740/782] Loss_D: 0.4278 Loss_G: 2.7411\n","[0/2][741/782] Loss_D: 0.6096 Loss_G: 4.3735\n","[0/2][742/782] Loss_D: 0.5670 Loss_G: 2.8633\n","[0/2][743/782] Loss_D: 0.3123 Loss_G: 2.5588\n","[0/2][744/782] Loss_D: 0.2795 Loss_G: 3.5788\n","[0/2][745/782] Loss_D: 0.2898 Loss_G: 3.4734\n","[0/2][746/782] Loss_D: 0.1181 Loss_G: 3.9056\n","[0/2][747/782] Loss_D: 0.2259 Loss_G: 3.3884\n","[0/2][748/782] Loss_D: 0.6493 Loss_G: 3.1689\n","[0/2][749/782] Loss_D: 1.2088 Loss_G: 1.8218\n","[0/2][750/782] Loss_D: 1.0023 Loss_G: 4.4415\n","[0/2][751/782] Loss_D: 0.9308 Loss_G: 2.2866\n","[0/2][752/782] Loss_D: 0.2590 Loss_G: 2.8874\n","[0/2][753/782] Loss_D: 0.5888 Loss_G: 4.4540\n","[0/2][754/782] Loss_D: 0.7413 Loss_G: 2.4453\n","[0/2][755/782] Loss_D: 0.4539 Loss_G: 3.5429\n","[0/2][756/782] Loss_D: 0.3669 Loss_G: 4.2404\n","[0/2][757/782] Loss_D: 0.3785 Loss_G: 3.2630\n","[0/2][758/782] Loss_D: 0.5521 Loss_G: 3.1646\n","[0/2][759/782] Loss_D: 0.4239 Loss_G: 4.0889\n","[0/2][760/782] Loss_D: 0.4056 Loss_G: 3.0304\n","[0/2][761/782] Loss_D: 0.2697 Loss_G: 3.1157\n","[0/2][762/782] Loss_D: 0.3506 Loss_G: 5.0114\n","[0/2][763/782] Loss_D: 0.2036 Loss_G: 4.3423\n","[0/2][764/782] Loss_D: 0.4830 Loss_G: 2.2890\n","[0/2][765/782] Loss_D: 0.3462 Loss_G: 3.8183\n","[0/2][766/782] Loss_D: 0.3131 Loss_G: 5.1579\n","[0/2][767/782] Loss_D: 0.2932 Loss_G: 3.7236\n","[0/2][768/782] Loss_D: 0.2708 Loss_G: 5.7395\n","[0/2][769/782] Loss_D: 0.1608 Loss_G: 4.8566\n","[0/2][770/782] Loss_D: 0.3923 Loss_G: 2.4896\n","[0/2][771/782] Loss_D: 1.2983 Loss_G: 8.2338\n","[0/2][772/782] Loss_D: 1.5003 Loss_G: 2.6944\n","[0/2][773/782] Loss_D: 0.5102 Loss_G: 4.3250\n","[0/2][774/782] Loss_D: 0.2456 Loss_G: 5.2542\n","[0/2][775/782] Loss_D: 0.5503 Loss_G: 5.0627\n","[0/2][776/782] Loss_D: 0.6345 Loss_G: 3.3530\n","[0/2][777/782] Loss_D: 0.9562 Loss_G: 4.7282\n","[0/2][778/782] Loss_D: 0.8201 Loss_G: 2.0784\n","[0/2][779/782] Loss_D: 0.7030 Loss_G: 6.4026\n","[0/2][780/782] Loss_D: 0.5479 Loss_G: 3.8807\n","[0/2][781/782] Loss_D: 0.3295 Loss_G: 5.4275\n","[1/2][0/782] Loss_D: 0.3597 Loss_G: 3.8696\n","[1/2][1/782] Loss_D: 0.7310 Loss_G: 4.5601\n","[1/2][2/782] Loss_D: 0.5177 Loss_G: 3.5129\n","[1/2][3/782] Loss_D: 0.9197 Loss_G: 5.1514\n","[1/2][4/782] Loss_D: 0.3883 Loss_G: 4.5126\n","[1/2][5/782] Loss_D: 0.3793 Loss_G: 3.0275\n","[1/2][6/782] Loss_D: 0.7055 Loss_G: 6.8397\n","[1/2][7/782] Loss_D: 0.8019 Loss_G: 3.2917\n","[1/2][8/782] Loss_D: 0.5500 Loss_G: 4.1607\n","[1/2][9/782] Loss_D: 0.2708 Loss_G: 4.8428\n","[1/2][10/782] Loss_D: 0.4191 Loss_G: 3.4742\n","[1/2][11/782] Loss_D: 1.0113 Loss_G: 8.6586\n","[1/2][12/782] Loss_D: 2.5300 Loss_G: 3.3639\n","[1/2][13/782] Loss_D: 0.6608 Loss_G: 4.4896\n","[1/2][14/782] Loss_D: 0.3550 Loss_G: 4.1717\n","[1/2][15/782] Loss_D: 0.3406 Loss_G: 3.9415\n","[1/2][16/782] Loss_D: 0.4853 Loss_G: 4.4440\n","[1/2][17/782] Loss_D: 0.3523 Loss_G: 4.0321\n","[1/2][18/782] Loss_D: 0.6653 Loss_G: 2.7235\n","[1/2][19/782] Loss_D: 1.2737 Loss_G: 7.4838\n","[1/2][20/782] Loss_D: 1.6312 Loss_G: 2.6953\n","[1/2][21/782] Loss_D: 0.5476 Loss_G: 3.9241\n","[1/2][22/782] Loss_D: 0.7325 Loss_G: 6.8314\n","[1/2][23/782] Loss_D: 0.6745 Loss_G: 3.9370\n","[1/2][24/782] Loss_D: 0.5759 Loss_G: 4.8608\n","[1/2][25/782] Loss_D: 0.3344 Loss_G: 3.9965\n","[1/2][26/782] Loss_D: 0.6965 Loss_G: 3.8868\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-5d47926b176d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0merrD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-47a29bdd6737>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We forward propagate the signal through the whole neural network of the discriminator defined by self.main.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We return the output which will be a value between 0 and 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"S-jEOFuNuSJI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPw-u6FluSMy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLRoiac8uSPw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBqfAUVJuSXY"},"source":[""],"execution_count":null,"outputs":[]}]}